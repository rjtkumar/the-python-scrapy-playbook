Virtual environment:
    Setup a python virtual environment
        > python3 -m venv venv
    Activate the python virtual environment
        > source venv/bin/activate
    Installing scrapy in the virtual environment
        > pip install scrapy

Creating a scrapy project:
    > scrapy startproject <project_name>
To create a generic spider:
    > scrapy genspider <spider_name> <website>
    > scrapy genspider chocolate https://www.chocolate.co.uk
To open scrapy shell:
    > scrapy shell
To set ipython as the default shell for scrapy shell update scrapy.cfg as shown:
    ## scrapy.cfg
    [settings]
    default = ...
    shell = ipython
To print a list of all spiders in the current project:
    > scrapy list
To run a spider:
    > scrapy crawl <spider_name>

Options while saving output to a file:
    -O: Overwrite any existing file or create new
    -o: Append to any existing file or create new
To save the output to a json file:
    Relative path:
        > scrapy crawl <spider_name> -O <output_file.json>
    Absolute path:
        > scrapy crawl <spider_name> -O <file:///home/.../output_file.json:json>
To save the output to a csv file:
    Relative path:
        > scrapy crawl <spider_name> -O <output_file.csv>
    Absolute path:
        > scrapy crawl <spider_name> -O <file:///home/.../output_file.csv:csv>
    
Saving to an Amazon S3 bubcket:
    Need to have botocore installed to be able to connect to theh S3 bubcket
        > pip install botocore
    Now that we have botocore installed we can save to S3:
        > scrapy crawl <spider_name> -O s3://aws_key:aws_secret@my_bucket/path/to/scraped_data.csv:csv
    We can also store the AWS ke and secret in the project settings:
        AWS_ACCESS_KEY_ID= <aws_key>
        AWS_SECRET_ACCESS_KEY= <aws-secret>
    The AWS feed exporter uses delayed file delivery, first the file is temporarily saved locally and then uploaded oncec the spider has completed the job

Saving to a mysql database:
    To connect our python script to the mysql database we'll use the library "mysql-connector-python"
        > pip install mysql-connector-python
Saving to a Postgres datbase:
    To connect to a postgres database we use the library 'psycopg2'
        > pip install psycopg2

Scrapy library for rotating user-agents;
        > pip install scrapy-user-agents
    Enabling the library in settings.py
        DOWNLOADER_MIDDLEWARES = {
            "scrapy.downloadermiddlewares.useragent.UserAgentMiddleware" : None ,# Disable the default user agent middleware
            "scrap_user_agents.middlewares.RandomUserAgentMiddleware" : 400
        }

To create a requirements.txt file:
    > pip freeze requirements.txt > <filename.txt>

Install the ML autopager
    > pip install autopager

A library to handle rotating proxies:
    > pip install scrapy-rotating-proxies