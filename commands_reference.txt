Virtual environment:
    Setup a python virtual environment
        > python3 -m venv venv
    Activate the python virtual environment
        > source venv/bin/activate
    Installing scrapy in the virtual environment
        > pip install scrapy

Creating a scrapy project:
    > scrapy startproject <project_name>
To create a generic spider:
    > scrapy genspider <spider_name> <website>
    > scrapy genspider chocolate https://www.chocolate.co.uk
To open scrapy shell:
    > scrapy shell
To set ipython as the default shell for scrapy shell update scrapy.cfg as shown:
    ## scrapy.cfg
    [settings]
    default = ...
    shell = ipython
To print a list of all spiders in the current project:
    > scrapy list
To run a spider:
    > scrapy crawl <spider_name>

Options while saving output to a file:
    -O: Overwrite any existing file or create new
    -o: Append to any existing file or create new
To save the output to a json file:
    Relative path:
        > scrapy crawl <spider_name> -O <output_file.json>
    Absolute path:
        > scrapy crawl <spider_name> -O <file:///home/.../output_file.json:json>
To save the output to a csv file:
    Relative path:
        > scrapy crawl <spider_name> -O <output_file.csv>
    Absolute path:
        > scrapy crawl <spider_name> -O <file:///home/.../output_file.csv:csv>
    
Saving to an Amazon S3 bubcket:
    Need to have botocore installed to be able to connect to theh S3 bubcket
        > pip install botocore
    Now that we have botocore installed we can save to S3:
        > scrapy crawl <spider_name> -O s3://aws_key:aws_secret@my_bucket/path/to/scraped_data.csv:csv
    We can also store the AWS ke and secret in the project settings:
        AWS_ACCESS_KEY_ID= <aws_key>
        AWS_SECRET_ACCESS_KEY= <aws-secret>
    The AWS feed exporter uses delayed file delivery, first the file is temporarily saved locally and then uploaded oncec the spider has completed the job

Saving to a mysql database:
    To connect our python script to the mysql database we'll use the library "mysql-connector-python"
        > pip install mysql-connector-python
Saving to a Postgres datbase:
    To connect to a postgres database we use the library 'psycopg2'
        > pip install psycopg2

Scrapy library for rotating user-agents;
        > pip install scrapy-user-agents
    Enabling the library in settings.py
        DOWNLOADER_MIDDLEWARES = {
            "scrapy.downloadermiddlewares.useragent.UserAgentMiddleware" : None ,# Disable the default user agent middleware
            "scrap_user_agents.middlewares.RandomUserAgentMiddleware" : 400
        }

To create a requirements.txt file:
    > pip freeze requirements.txt > <filename.txt>

Install the ML autopager
    > pip install autopager

A library to handle rotating proxies:
    > pip install scrapy-rotating-proxies

Saving scraped data to file:
-o : Appends new data to existing file
-O : Overwrites existing file
    JSON and JSON Lines
        Via command line arguments:
            JSON: 
                > scrapy crawl <spider_name> -o <filename.json> 
            JSON Lines:
                > scrapy crawl <spider_name> -o <filename.jsonl>
        JSON doesn't support stream mode parsing, The entire data is stored in memory and saved at the end of the crawl which can cause memory leaks.
        To overcome this we may use JOSN-lines which allows for incremental parsing.